{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h2><font size=\"5\"> CONVOLUTIONAL NEURAL NETWORK APPLICATION</font></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio==1.24.3\n",
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if not tf.__version__ == '2.2.0':\n",
    "    printmd('<<<<<!!!!! ERROR !!!! please upgrade to TensorFlow 2.2.0, or restart your Kernel (Kernel->Restart & Clear Output)>>>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the MNIST dataset using TensorFlow built-in feature</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"categorical labels\")\n",
    "print(y_train[0:5])\n",
    "\n",
    "# make labels one hot encoded\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "y_test = tf.one_hot(y_test, 10)\n",
    "\n",
    "print(\"one hot encoded labels\")\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training examples:\" , x_train.shape[0])\n",
    "print(\"number of test examples:\" , x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(50)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting a 2D Image into a 1D Vector</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing an example of the Flatten class and operation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "flatten = Flatten(dtype='float32')\n",
    "\n",
    "\"original data shape\"\n",
    "print(x_train.shape)\n",
    "\n",
    "\"flattened shape\"\n",
    "print(flatten(x_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight tensor\n",
    "W = tf.Variable(tf.zeros([784, 10], tf.float32))\n",
    "# Bias tensor\n",
    "b = tf.Variable(tf.zeros([10], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample softmax calculation on an input vector\n",
    "vector = [10, 0.2, 8]\n",
    "softmax = tf.nn.softmax(vector)\n",
    "print(\"softmax calculation\")\n",
    "print(softmax.numpy())\n",
    "print(\"verifying normalization\")\n",
    "print(tf.reduce_sum(softmax))\n",
    "print(\"finding vector with largest value (label assignment)\")\n",
    "print(\"category\", tf.argmax(softmax).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return tf.nn.softmax(forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    x = flatten(x)\n",
    "    return activate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_label, y_pred):\n",
    "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))\n",
    "# addition of 1e-10 to prevent errors in zero calculations\n",
    "\n",
    "# current loss function for unoptimized model\n",
    "cross_entropy(y_train, model(x_train)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Type of optimization: Gradient Descent</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y ):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #compute loss function\n",
    "        current_loss = cross_entropy( y, model(x))\n",
    "        # compute gradient of loss \n",
    "        #(This is automatic! Even with specialized funcctions!)\n",
    "        grads = tape.gradient( current_loss , [W,b] )\n",
    "        # Apply SGD step to our Variables W and b\n",
    "        optimizer.apply_gradients( zip( grads , [W,b] ) )     \n",
    "    return current_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training batches</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeroing out weights in case you want to run this cell multiple times\n",
    "# Weight tensor\n",
    "W = tf.Variable(tf.zeros([784, 10],tf.float32))\n",
    "# Bias tensor\n",
    "b = tf.Variable(tf.zeros([10],tf.float32))\n",
    "\n",
    "loss_values=[]\n",
    "accuracies = []\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    j=0\n",
    "    # each batch has 50 examples\n",
    "    for x_train_batch, y_train_batch in train_ds:\n",
    "        j+=1\n",
    "        current_loss = train_step(x_train_batch, y_train_batch)\n",
    "        if j%500==0: #reporting intermittent batch statistics\n",
    "            print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss) ) \n",
    "    \n",
    "    # collecting statistics at each epoch...loss function and accuracy\n",
    "    #  loss function\n",
    "    current_loss = cross_entropy( y_train, model( x_train )).numpy()\n",
    "    loss_values.append(current_loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(model(x_train), axis=1),\n",
    "                                  tf.argmax(y_train, axis=1))\n",
    "    #  accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"end of epoch \", str(i), \"loss\", str(current_loss), \"accuracy\", str(accuracy) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test and Plots</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction_train = tf.equal(tf.argmax(model(x_train), axis=1),tf.argmax(y_train,axis=1))\n",
    "accuracy_train = tf.reduce_mean(tf.cast(correct_prediction_train, tf.float32)).numpy()\n",
    "\n",
    "correct_prediction_test = tf.equal(tf.argmax(model(x_test), axis=1),tf.argmax(y_test, axis=1))\n",
    "accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32)).numpy()\n",
    "\n",
    "print(\"training accuracy\", accuracy_train)\n",
    "print(\"test accuracy\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "#print(loss_values)\n",
    "plt.plot(loss_values,'-ro')\n",
    "plt.title(\"loss per epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies,'-ro')\n",
    "plt.title(\"accuracy per epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "\n",
    "<h2>Evaluating the final result</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Converting images of the data set to tensors</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image_train = tf.reshape(x_train, [-1,28,28,1])  \n",
    "x_image_train = tf.cast(x_image_train, 'float32') \n",
    "\n",
    "x_image_test = tf.reshape(x_test, [-1,28,28,1]) \n",
    "x_image_test = tf.cast(x_image_test, 'float32') \n",
    "\n",
    "#creating new dataset with reshaped inputs\n",
    "train_ds2 = tf.data.Dataset.from_tensor_slices((x_image_train, y_train)).batch(50)\n",
    "test_ds2 = tf.data.Dataset.from_tensor_slices((x_image_test, y_test)).batch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image_train = tf.slice(x_image_train,[0,0,0,0],[10000, 28, 28, 1])\n",
    "y_train = tf.slice(y_train,[0,0],[10000, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional Layer 1</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev=0.1, seed=0))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve1(x):\n",
    "    return(\n",
    "        tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the ReLU activation Function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_conv1(x): return(tf.nn.relu(convolve1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the max pooling</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1(x):\n",
    "    return tf.nn.max_pool(h_conv1(x), ksize=[1, 2, 2, 1], \n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional Layer 2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convolve image with weight tensor and add biases.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2(x): \n",
    "    return( \n",
    "    tf.nn.conv2d(conv1(x), W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the ReLU activation Function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_conv2(x):  return tf.nn.relu(convolve2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the max pooling</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2(x):  \n",
    "    return(\n",
    "    tf.nn.max_pool(h_conv2(x), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fully Connected Layer</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Flattening Second Layer</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer2_matrix(x): return tf.reshape(conv2(x), [-1, 7 * 7 * 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weights and Biases between layer 2 and 3</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1, seed = 2))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # need 1024 biases for 1024 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Matrix Multiplication (applying weights and biases)</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcl(x): return tf.matmul(layer2_matrix(x), W_fc1) + b_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the ReLU activation Function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_fc1(x): return tf.nn.relu(fcl(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dropout Layer, Optional phase for reducing overfitting</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob=0.5\n",
    "def layer_drop(x): return tf.nn.dropout(h_fc1(x), keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weights and Biases</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1, seed = 2)) #1024 neurons\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Matrix Multiplication (applying weights and biases)</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x): return tf.matmul(layer_drop(x), W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the Softmax activation Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_CNN(x): return tf.nn.softmax(fc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define the loss function</h4>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "layer4_test =[[0.9, 0.1, 0.1],[0.9, 0.1, 0.1]]\n",
    "y_test=[[1.0, 0.0, 0.0],[1.0, 0.0, 0.0]]\n",
    "np.mean( -np.sum(y_test * np.log(layer4_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_label, y_pred):\n",
    "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define the optimizer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [W_conv1, b_conv1, W_conv2, b_conv2, \n",
    "             W_fc1, b_fc1, W_fc2, b_fc2, ]\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = cross_entropy( y, y_CNN( x ))\n",
    "        grads = tape.gradient( current_loss , variables )\n",
    "        optimizer.apply_gradients( zip( grads , variables ) )\n",
    "        return current_loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"results = []\n",
    "increment = 1000\n",
    "for start in range(0,60000,increment):\n",
    "    s = tf.slice(x_image_train,[start,0,0,0],[start+increment-1, 28, 28, 1])\n",
    "    t = y_CNN(s)\n",
    "    #results.append(t)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define prediction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1), tf.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Define accuracy</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run session, train</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values=[]\n",
    "accuracies = []\n",
    "epochs = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "    j=0\n",
    "    # each batch has 50 examples\n",
    "    for x_train_batch, y_train_batch in train_ds2:\n",
    "        j+=1\n",
    "        current_loss = train_step(x_train_batch, y_train_batch)\n",
    "        if j%50==0: #reporting intermittent batch statistics\n",
    "            correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
    "                                  tf.argmax(y_train_batch, axis=1))\n",
    "            #  accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "            print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss),\n",
    "                     \"accuracy\", str(accuracy)) \n",
    "            \n",
    "    current_loss = cross_entropy( y_train, y_CNN( x_image_train )).numpy()\n",
    "    loss_values.append(current_loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1),\n",
    "                                  tf.argmax(y_train, axis=1))\n",
    "    #  accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "    accuracies.append(accuracy)\n",
    "    print(\"end of epoch \", str(i), \"loss\", str(current_loss), \"accuracy\", str(accuracy) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref9\"></a>\n",
    "\n",
    "<h2>Evaluate the model</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "acccuracies=[]\n",
    "# evaluate accuracy by batch and average...reporting every 100th batch\n",
    "for x_train_batch, y_train_batch in train_ds2:\n",
    "        j+=1\n",
    "        correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
    "                                  tf.argmax(y_train_batch, axis=1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "        #accuracies.append(accuracy)\n",
    "        if j%100==0:\n",
    "            print(\"batch\", str(j), \"accuracy\", str(accuracy) ) \n",
    "import numpy as np\n",
    "print(\"accuracy of entire set\", str(np.mean(accuracies)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualization</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = tf.reshape(tf.transpose(W_conv1, perm=[2, 3, 0,1]),[32, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --output-document utils1.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week2/data/utils.py\n",
    "import utils1\n",
    "import imp\n",
    "imp.reload(utils1)\n",
    "from utils1 import tile_raster_images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "image = Image.fromarray(tile_raster_images(kernels.numpy(), img_shape=(5, 5) ,tile_shape=(4, 8), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "sampleimage = [x_image_train[0]]\n",
    "plt.imshow(np.reshape(sampleimage,[28,28]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ActivatedUnits = sess.run(convolve1,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
    "keep_prob=1.0\n",
    "ActivatedUnits = convolve1(sampleimage)\n",
    "                           \n",
    "filters = ActivatedUnits.shape[3]\n",
    "plt.figure(1, figsize=(20,20))\n",
    "n_columns = 6\n",
    "n_rows = np.math.ceil(filters / n_columns) + 1\n",
    "for i in range(filters):\n",
    "    plt.subplot(n_rows, n_columns, i+1)\n",
    "    plt.title('Filter ' + str(i))\n",
    "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ActivatedUnits = sess.run(convolve2,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
    "ActivatedUnits = convolve2(sampleimage)\n",
    "filters = ActivatedUnits.shape[3]\n",
    "plt.figure(1, figsize=(20,20))\n",
    "n_columns = 8\n",
    "n_rows = np.math.ceil(filters / n_columns) + 1\n",
    "for i in range(filters):\n",
    "    plt.subplot(n_rows, n_columns, i+1)\n",
    "    plt.title('Filter ' + str(i))\n",
    "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
